b'<?xml version="1.0" ?>\n<!DOCTYPE PubmedArticleSet PUBLIC "-//NLM//DTD PubMedArticle, 1st January 2019//EN" "https://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_190101.dtd">\n<PubmedArticleSet>\n<PubmedArticle>\n    <MedlineCitation Status="In-Process" Owner="NLM">\n        <PMID Version="1">31639622</PMID>\n        <DateRevised>\n            <Year>2020</Year>\n            <Month>02</Month>\n            <Day>26</Day>\n        </DateRevised>\n        <Article PubModel="Print-Electronic">\n            <Journal>\n                <ISSN IssnType="Electronic">1361-8423</ISSN>\n                <JournalIssue CitedMedium="Internet">\n                    <Volume>59</Volume>\n                    <PubDate>\n                        <Year>2020</Year>\n                        <Month>01</Month>\n                    </PubDate>\n                </JournalIssue>\n                <Title>Medical image analysis</Title>\n                <ISOAbbreviation>Med Image Anal</ISOAbbreviation>\n            </Journal>\n            <ArticleTitle>Multi-task recurrent convolutional network with correlation loss for surgical video analysis.</ArticleTitle>\n            <Pagination>\n                <MedlinePgn>101572</MedlinePgn>\n            </Pagination>\n            <ELocationID EIdType="pii" ValidYN="Y">S1361-8415(19)30112-4</ELocationID>\n            <ELocationID EIdType="doi" ValidYN="Y">10.1016/j.media.2019.101572</ELocationID>\n            <Abstract>\n                <AbstractText>Surgical tool presence detection and surgical phase recognition are two fundamental yet challenging tasks in surgical video analysis as well as very essential components in various applications in modern operating rooms. While these two analysis tasks are highly correlated in clinical practice as the surgical process is typically well-defined, most previous methods tackled them separately, without making full use of their relatedness. In this paper, we present a novel method by developing a multi-task recurrent convolutional network with correlation loss (MTRCNet-CL) to exploit their relatedness to simultaneously boost the performance of both tasks. Specifically, our proposed MTRCNet-CL model has an end-to-end architecture with two branches, which share earlier feature encoders to extract general visual features while holding respective higher layers targeting for specific tasks. Given that temporal information is crucial for phase recognition, long-short term memory (LSTM) is explored to model the sequential dependencies in the phase recognition branch. More importantly, a novel and effective correlation loss is designed to model the relatedness between tool presence and phase identification of each video frame, by minimizing the divergence of predictions from the two branches. Mutually leveraging both low-level feature sharing and high-level prediction correlating, our MTRCNet-CL method can encourage the interactions between the two tasks to a large extent, and hence can bring about benefits to each other. Extensive experiments on a large surgical video dataset (Cholec80) demonstrate outstanding performance of our proposed method, consistently exceeding the state-of-the-art methods by a large margin, e.g., 89.1%\xe2\x80\x89v.s.\xe2\x80\x8981.0% for the mAP in tool presence detection and 87.4%\xe2\x80\x89v.s.\xe2\x80\x8984.5% for F1 score in phase recognition.</AbstractText>\n                <CopyrightInformation>Copyright \xc2\xa9 2019 Elsevier B.V. All rights reserved.</CopyrightInformation>\n            </Abstract>\n            <AuthorList CompleteYN="Y">\n                <Author ValidYN="Y">\n                    <LastName>Jin</LastName>\n                    <ForeName>Yueming</ForeName>\n                    <Initials>Y</Initials>\n                    <AffiliationInfo>\n                        <Affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong, China.</Affiliation>\n                    </AffiliationInfo>\n                </Author>\n                <Author ValidYN="Y">\n                    <LastName>Li</LastName>\n                    <ForeName>Huaxia</ForeName>\n                    <Initials>H</Initials>\n                    <AffiliationInfo>\n                        <Affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong, China.</Affiliation>\n                    </AffiliationInfo>\n                </Author>\n                <Author ValidYN="Y">\n                    <LastName>Dou</LastName>\n                    <ForeName>Qi</ForeName>\n                    <Initials>Q</Initials>\n                    <AffiliationInfo>\n                        <Affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong, China. Electronic address: qdou@cse.cuhk.edu.hk.</Affiliation>\n                    </AffiliationInfo>\n                </Author>\n                <Author ValidYN="Y">\n                    <LastName>Chen</LastName>\n                    <ForeName>Hao</ForeName>\n                    <Initials>H</Initials>\n                    <AffiliationInfo>\n                        <Affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong, China.</Affiliation>\n                    </AffiliationInfo>\n                </Author>\n                <Author ValidYN="Y">\n                    <LastName>Qin</LastName>\n                    <ForeName>Jing</ForeName>\n                    <Initials>J</Initials>\n                    <AffiliationInfo>\n                        <Affiliation>Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic University, China.</Affiliation>\n                    </AffiliationInfo>\n                </Author>\n                <Author ValidYN="Y">\n                    <LastName>Fu</LastName>\n                    <ForeName>Chi-Wing</ForeName>\n                    <Initials>CW</Initials>\n                    <AffiliationInfo>\n                        <Affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong, China.</Affiliation>\n                    </AffiliationInfo>\n                </Author>\n                <Author ValidYN="Y">\n                    <LastName>Heng</LastName>\n                    <ForeName>Pheng-Ann</ForeName>\n                    <Initials>PA</Initials>\n                    <AffiliationInfo>\n                        <Affiliation>Department of Computer Science and Engineering, The Chinese University of Hong Kong, China; T Stone Robotics Institute, The Chinese University of Hong Kong, China.</Affiliation>\n                    </AffiliationInfo>\n                </Author>\n            </AuthorList>\n            <Language>eng</Language>\n            <PublicationTypeList>\n                <PublicationType UI="D016428">Journal Article</PublicationType>\n                <PublicationType UI="D013485">Research Support, Non-U.S. Gov\'t</PublicationType>\n            </PublicationTypeList>\n            <ArticleDate DateType="Electronic">\n                <Year>2019</Year>\n                <Month>10</Month>\n                <Day>10</Day>\n            </ArticleDate>\n        </Article>\n        <MedlineJournalInfo>\n            <Country>Netherlands</Country>\n            <MedlineTA>Med Image Anal</MedlineTA>\n            <NlmUniqueID>9713490</NlmUniqueID>\n            <ISSNLinking>1361-8415</ISSNLinking>\n        </MedlineJournalInfo>\n        <CitationSubset>IM</CitationSubset>\n        <KeywordList Owner="NOTNLM">\n            <Keyword MajorTopicYN="Y">Correlation loss</Keyword>\n            <Keyword MajorTopicYN="Y">Deep learning</Keyword>\n            <Keyword MajorTopicYN="Y">Multi-task learning</Keyword>\n            <Keyword MajorTopicYN="Y">Surgical video analysis</Keyword>\n        </KeywordList>\n    </MedlineCitation>\n    <PubmedData>\n        <History>\n            <PubMedPubDate PubStatus="received">\n                <Year>2018</Year>\n                <Month>11</Month>\n                <Day>08</Day>\n            </PubMedPubDate>\n            <PubMedPubDate PubStatus="revised">\n                <Year>2019</Year>\n                <Month>09</Month>\n                <Day>29</Day>\n            </PubMedPubDate>\n            <PubMedPubDate PubStatus="accepted">\n                <Year>2019</Year>\n                <Month>10</Month>\n                <Day>03</Day>\n            </PubMedPubDate>\n            <PubMedPubDate PubStatus="pubmed">\n                <Year>2019</Year>\n                <Month>10</Month>\n                <Day>23</Day>\n                <Hour>6</Hour>\n                <Minute>0</Minute>\n            </PubMedPubDate>\n            <PubMedPubDate PubStatus="medline">\n                <Year>2019</Year>\n                <Month>10</Month>\n                <Day>23</Day>\n                <Hour>6</Hour>\n                <Minute>0</Minute>\n            </PubMedPubDate>\n            <PubMedPubDate PubStatus="entrez">\n                <Year>2019</Year>\n                <Month>10</Month>\n                <Day>23</Day>\n                <Hour>6</Hour>\n                <Minute>0</Minute>\n            </PubMedPubDate>\n        </History>\n        <PublicationStatus>ppublish</PublicationStatus>\n        <ArticleIdList>\n            <ArticleId IdType="pubmed">31639622</ArticleId>\n            <ArticleId IdType="pii">S1361-8415(19)30112-4</ArticleId>\n            <ArticleId IdType="doi">10.1016/j.media.2019.101572</ArticleId>\n        </ArticleIdList>\n    </PubmedData>\n</PubmedArticle>\n\n</PubmedArticleSet>'